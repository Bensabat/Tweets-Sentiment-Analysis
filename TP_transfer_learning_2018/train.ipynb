{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet sentiment analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o_O', '😮', 'ok', '😉', 'hhh', '☺️']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "\n",
    "from preprocessing import data_preprocessing, data_preprocessing_test\n",
    "from collections import Counter\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import LSTM, Dropout, Dense, Bidirectional,  Flatten, Input, GRU\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "\n",
    "#from preprocessing import standardization, data_preprocessing\n",
    "#mpl.use('TkAgg')  # or whatever other backend that you want\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data (word embedding and tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = \"data/GoogleNews-vectors-negative300.bin\"\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "corpora_train_3 = \"data/task_A/data_train_3.csv\"\n",
    "corpora_train_7 = \"data/task_A/data_train_7.csv\"\n",
    "\n",
    "corpora_test_7 = \"data/task_A/data_test_7.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "\"Won the match #getin . Plus\\u002c tomorrow is a very busy day\\u002c with Awareness Day\\u2019s and debates. Gulp. Debates...\" ->\n",
    "\n",
    "\"win match plus tomorrow busy day awareness day debate gulp debate ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_3, sentiments_train_3 = data_preprocessing(corpora_train_3, 'train')\n",
    "tweets_train_7, sentiments_train_7 = data_preprocessing(corpora_train_7, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting word index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the two list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweet = tweets_train_3.append(tweets_train_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize tweets and get the word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters=' ')\n",
    "tokenizer.fit_on_texts(all_tweet)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: \"win match plus tomorrow busy day awareness day debate gulp debate ...\" -> [43, 234, 769, 2, 1054, 6, 2896, 6, 424, 12301, 424, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train_3 = tokenizer.texts_to_sequences(tweets_train_3)\n",
    "sequences_train_7 = tokenizer.texts_to_sequences(tweets_train_7)\n",
    "\n",
    "sequences = sequences_train_3 + sequences_train_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the max tweet sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 0\n",
    "for elt in sequences:\n",
    "    if len(elt) > MAX_SEQUENCE_LENGTH:\n",
    "        MAX_SEQUENCE_LENGTH = len(elt)\n",
    "\n",
    "print(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37785, 300)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the embedded matrix with weight from embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov = []\n",
    "oov.append((np.random.rand(EMBEDDING_DIM) * 2.0) - 1.0)\n",
    "oov = oov / np.linalg.norm(oov)\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "    else:\n",
    "        embedding_matrix[i] = oov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_3 = pad_sequences(sequences_train_3, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_train_7 = pad_sequences(sequences_train_7, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_train_3 = np.arange(data_train_3.shape[0])\n",
    "#data_train_3 = data_train_3[indices_train_3]\n",
    "\n",
    "labels_train_3 = to_categorical(np.asarray(sentiments_train_3), 3)\n",
    "#labels_train_3 = labels_train_3[indices_train_3]\n",
    "\n",
    "labels_train_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_train_7 = np.arange(data_train_7.shape[0])\n",
    "#data_train_7 = data_train_7[indices_train_7]\n",
    "\n",
    "labels_train_7 = to_categorical(np.asarray(sentiments_train_7), 7)\n",
    "#labels_train_7 = labels_train_7[indices_train_7]\n",
    "\n",
    "labels_train_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 35233 samples\n",
      "validation set: 15100 samples\n",
      "x_train_3 shape:  (35233, 32)\n",
      "y_train_3 shape:  (35233, 3)\n"
     ]
    }
   ],
   "source": [
    "split_idx = int(len(data_train_3) * 0.70)\n",
    "\n",
    "x_train_3, x_val_3 = data_train_3[:split_idx], data_train_3[split_idx:]\n",
    "y_train_3, y_val_3 = labels_train_3 [:split_idx], labels_train_3[split_idx:]\n",
    "\n",
    "print('training set: ' + str(len(x_train_3)) + ' samples')\n",
    "print('validation set: ' + str(len(x_val_3)) + ' samples')\n",
    "# print('test set: ' + str(len(x_test)) + ' samples')\n",
    "\n",
    "print('x_train_3 shape: ', x_train_3.shape)\n",
    "print('y_train_3 shape: ', y_train_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 1385 samples\n",
      "validation set: 245 samples\n",
      "x_train_7 shape:  (1385, 32)\n",
      "y_train_7 shape:  (1385, 7)\n"
     ]
    }
   ],
   "source": [
    "split_idx = int(len(data_train_7) * 0.85)\n",
    "\n",
    "x_train_7, x_val_7 = data_train_7[:split_idx], data_train_7[split_idx:]\n",
    "y_train_7, y_val_7 = labels_train_7 [:split_idx], labels_train_7[split_idx:]\n",
    "\n",
    "print('training set: ' + str(len(x_train_7)) + ' samples')\n",
    "print('validation set: ' + str(len(x_val_7)) + ' samples')\n",
    "\n",
    "print('x_train_7 shape: ', x_train_7.shape)\n",
    "print('y_train_7 shape: ', y_train_7.shape)\n",
    "\n",
    "#, y_train_7, x_val_7,y_train_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False, name='embedding_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(x_train_3, y_train_3, x_val_3, y_val_3, embedding_layer):\n",
    "\n",
    "    model1 = Sequential()\n",
    "    \n",
    "    model1.add(embedding_layer)\n",
    "    model1.add(LSTM(32))\n",
    "    model1.add(Dropout(0.2))\n",
    "    model1.add(Dense(32, activation='relu'))\n",
    "    model1.add(Dropout(0.2))\n",
    "    model1.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    model1.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "    model1.summary()\n",
    "    history = model1.fit(x_train_3, y_train_3, validation_data=(x_val_3, y_val_3),epochs=6, batch_size=50)\n",
    "    \n",
    "    model1.save(\"./model1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer,epochs, batch_size):\n",
    "\n",
    "    model2 = Sequential()\n",
    "    \n",
    "    model2.add(embedding_layer)\n",
    "    model2.add(GRU(32))\n",
    "    model2.add(Dropout(0.2))\n",
    "    model2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model2.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "    model2.summary()\n",
    "    history = model2.fit(x_train_3, y_train_3, validation_data=(x_val_3, y_val_3), epochs=6, batch_size=50)\n",
    "\n",
    "    model2.save(\"./model2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 32, 300)           11335500  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 11,379,279\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 11,335,500\n",
      "_________________________________________________________________\n",
      "Train on 35233 samples, validate on 15100 samples\n",
      "Epoch 1/6\n",
      "35233/35233 [==============================] - 18s 498us/step - loss: 1.0405 - acc: 0.4648 - val_loss: 1.0337 - val_acc: 0.4085\n",
      "Epoch 2/6\n",
      "35233/35233 [==============================] - 16s 452us/step - loss: 1.0108 - acc: 0.4661 - val_loss: 1.0332 - val_acc: 0.4085\n",
      "Epoch 3/6\n",
      "35233/35233 [==============================] - 16s 453us/step - loss: 1.0092 - acc: 0.4661 - val_loss: 1.0329 - val_acc: 0.4085\n",
      "Epoch 4/6\n",
      "35233/35233 [==============================] - 16s 451us/step - loss: 1.0092 - acc: 0.4661 - val_loss: 1.0347 - val_acc: 0.4085\n",
      "Epoch 5/6\n",
      "35233/35233 [==============================] - 16s 455us/step - loss: 1.0092 - acc: 0.4661 - val_loss: 1.0349 - val_acc: 0.4085\n",
      "Epoch 6/6\n",
      "35233/35233 [==============================] - 16s 453us/step - loss: 1.0092 - acc: 0.4661 - val_loss: 1.0345 - val_acc: 0.4085\n"
     ]
    }
   ],
   "source": [
    "model1(x_train_3, y_train_3, x_val_3, y_val_3, embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfert learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./model1.h5\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut 2 last layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "#model.outputs = [model.layers[-1].output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(150, activation='relu', name='dense1'))\n",
    "model.add(Dense(64, activation='relu', name='dense2'))\n",
    "model.add(Dense(7, activation='softmax', name='dense3'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train_7, y_train_7, validation_data=(x_val_7, y_val_7), epochs=11, batch_size=50)\n",
    "model.save(\"./model3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing new transfert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test_7 = data_preprocessing_test(corpora_test_7)\n",
    "sequences_test_7 = tokenizer.texts_to_sequences(tweets_test_7)\n",
    "data_test_7 = pad_sequences(sequences_test_7, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "r = model.predict(data_test_7)\n",
    "data = pd.read_csv(\"/home/abdou/Documents/datastories-semeval2017-task4-master/dataset/Subtask_A/gold/2018-Valence-oc-En-test-gold.txt\", sep='\\t', encoding='utf-8')\n",
    "\n",
    "for i in range(len(r)):\n",
    "    data['Intensity Class'][i]=[\"-1: slightly negative emotional state can be inferred\",\n",
    "                                \"-2: moderately negative emotional state can be inferred\",\n",
    "                                \"-3: very negative emotional state can be inferred\",\n",
    "                                \"0: neutral or mixed emotional state can be inferred\",\n",
    "                                \"1: slightly positive emotional state can be inferred\",\n",
    "                                \"2: moderately positive emotional state can be inferred\",\n",
    "                                \"3: very positive emotional state can be inferred\"][numpy.argmax(r[i])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write predicting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./submission_after.csv\", \"w\")\n",
    "f.write(\"ID\tTweet\tAffect Dimension\tIntensity Class\\n\")\n",
    "\n",
    "for d in range(len(data)):\n",
    "    f.write(data['ID'][d] + \"\\t\" + data['Tweet'][d] + \"\\tvalence\\t\" + data['Intensity Class'][d] + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
