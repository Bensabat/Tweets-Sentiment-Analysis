{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet sentiment analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- (Embedding tweet instead of google news)\n",
    "- Add OLE on embedding (+2 into dim)\n",
    "- Add EV on embedding (+11 into dim)\n",
    "- \n",
    "- Cross validation\n",
    "- Comment v√©rifier l'overfitting?\n",
    "- Jouer avec hyperparametres (dropout, fonction activation, nb de neuronnes, nb de layers, nb d'epoche / batch, optimisation, architecture, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "\n",
    "from preprocessing import data_preprocessing, data_preprocessing_test\n",
    "from collections import Counter\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import LSTM, Dropout, Dense, Bidirectional,  Flatten, Input, GRU\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "\n",
    "#from preprocessing import standardization, data_preprocessing\n",
    "#mpl.use('TkAgg')  # or whatever other backend that you want\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data (word embedding and tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = \"data/GoogleNews-vectors-negative300.bin\"\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "corpora_train_3 = \"data/task_A/data_train_3.csv\"\n",
    "corpora_train_7 = \"data/task_A/data_train_7.csv\"\n",
    "\n",
    "corpora_test_7 = \"data/task_A/data_test_7.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "\"Won the match #getin . Plus\\u002c tomorrow is a very busy day\\u002c with Awareness Day\\u2019s and debates. Gulp. Debates...\" ->\n",
    "\n",
    "\"win match plus tomorrow busy day awareness day debate gulp debate ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_3, sentiments_train_3 = data_preprocessing(corpora_train_3, 'train')\n",
    "tweets_train_7, sentiments_train_7 = data_preprocessing(corpora_train_7, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting word index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the two list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweet = tweets_train_3.append(tweets_train_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize tweets and get the word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'...': 1,\n",
       " 'tomorrow': 2,\n",
       " 'not': 3,\n",
       " 'may': 4,\n",
       " 'go': 5,\n",
       " 'day': 6,\n",
       " 'get': 7,\n",
       " 'see': 8,\n",
       " 'night': 9,\n",
       " 'friday': 10,\n",
       " 'time': 11,\n",
       " 'like': 12,\n",
       " 'sunday': 13,\n",
       " '1st': 14,\n",
       " 'come': 15,\n",
       " 'make': 16,\n",
       " 'watch': 17,\n",
       " 'saturday': 18,\n",
       " 'one': 19,\n",
       " 'u': 20,\n",
       " 'say': 21,\n",
       " 'game': 22,\n",
       " '2nd': 23,\n",
       " 'new': 24,\n",
       " 'want': 25,\n",
       " 'monday': 26,\n",
       " 'think': 27,\n",
       " 'know': 28,\n",
       " 'today': 29,\n",
       " 'good': 30,\n",
       " 'would': 31,\n",
       " '3rd': 32,\n",
       " 'no': 33,\n",
       " 'tonight': 34,\n",
       " 'play': 35,\n",
       " 'love': 36,\n",
       " '..': 37,\n",
       " 'thursday': 38,\n",
       " 'back': 39,\n",
       " 'last': 40,\n",
       " 'take': 41,\n",
       " 'look': 42,\n",
       " 'win': 43,\n",
       " 'show': 44,\n",
       " 'still': 45,\n",
       " 'start': 46,\n",
       " 'sun': 47,\n",
       " 'best': 48,\n",
       " 'first': 49,\n",
       " 'Ô∏è': 50,\n",
       " 'wait': 51,\n",
       " 'cannot': 52,\n",
       " 'david': 53,\n",
       " 'next': 54,\n",
       " 'happy': 55,\n",
       " '4th': 56,\n",
       " 'big': 57,\n",
       " 'need': 58,\n",
       " 'tuesday': 59,\n",
       " 'work': 60,\n",
       " 'great': 61,\n",
       " 'year': 62,\n",
       " 'live': 63,\n",
       " '‚ò∫': 64,\n",
       " 'right': 65,\n",
       " 'miss': 66,\n",
       " 'world': 67,\n",
       " 'morning': 68,\n",
       " 'well': 69,\n",
       " 'give': 70,\n",
       " 'v': 71,\n",
       " 'wednesday': 72,\n",
       " 'ticket': 73,\n",
       " 'gonna': 74,\n",
       " 'hope': 75,\n",
       " 'really': 76,\n",
       " 'even': 77,\n",
       " 'season': 78,\n",
       " 'people': 79,\n",
       " 'run': 80,\n",
       " 'thing': 81,\n",
       " 'sit': 82,\n",
       " 'let': 83,\n",
       " 'birthday': 84,\n",
       " 'john': 85,\n",
       " 'lol': 86,\n",
       " 'way': 87,\n",
       " 'call': 88,\n",
       " 'better': 89,\n",
       " 'end': 90,\n",
       " 'guy': 91,\n",
       " 'man': 92,\n",
       " 'black': 93,\n",
       " 'star': 94,\n",
       " 'set': 95,\n",
       " 'could': 96,\n",
       " 'movie': 97,\n",
       " 'talk': 98,\n",
       " 'ever': 99,\n",
       " 'week': 100,\n",
       " 'open': 101,\n",
       " 'september': 102,\n",
       " 'concert': 103,\n",
       " 'much': 104,\n",
       " 'try': 105,\n",
       " 'tell': 106,\n",
       " 'album': 107,\n",
       " 'team': 108,\n",
       " 'march': 109,\n",
       " 'since': 110,\n",
       " 'never': 111,\n",
       " 'fan': 112,\n",
       " '5th': 113,\n",
       " 'find': 114,\n",
       " 'october': 115,\n",
       " 'place': 116,\n",
       " 'white': 117,\n",
       " 'top': 118,\n",
       " 'leave': 119,\n",
       " 'real': 120,\n",
       " 'video': 121,\n",
       " 'milan': 122,\n",
       " 'song': 123,\n",
       " 'july': 124,\n",
       " 'frank': 125,\n",
       " 'paul': 126,\n",
       " 'c': 127,\n",
       " 'august': 128,\n",
       " 'rt': 129,\n",
       " 'home': 130,\n",
       " 'november': 131,\n",
       " 'life': 132,\n",
       " 'meet': 133,\n",
       " 'george': 134,\n",
       " 'ice': 135,\n",
       " 'party': 136,\n",
       " 'also': 137,\n",
       " 'free': 138,\n",
       " 'buy': 139,\n",
       " 'hear': 140,\n",
       " 'school': 141,\n",
       " 'sure': 142,\n",
       " 'feel': 143,\n",
       " 'red': 144,\n",
       " 'national': 145,\n",
       " 'sox': 146,\n",
       " 'two': 147,\n",
       " 'bad': 148,\n",
       " 'please': 149,\n",
       " 'every': 150,\n",
       " 'news': 151,\n",
       " 'weekend': 152,\n",
       " 'listen': 153,\n",
       " 'anyone': 154,\n",
       " 'kendrick': 155,\n",
       " 'mean': 156,\n",
       " 'hit': 157,\n",
       " 'im': 158,\n",
       " 'check': 159,\n",
       " 'ready': 160,\n",
       " 'lead': 161,\n",
       " 'fuck': 162,\n",
       " 'w': 163,\n",
       " 'pm': 164,\n",
       " 'friend': 165,\n",
       " 'january': 166,\n",
       " 'tom': 167,\n",
       " 'girl': 168,\n",
       " 'release': 169,\n",
       " 'beat': 170,\n",
       " 'break': 171,\n",
       " 'remember': 172,\n",
       " 'west': 173,\n",
       " 'apple': 174,\n",
       " 'hour': 175,\n",
       " 'lose': 176,\n",
       " 'super': 177,\n",
       " 'chris': 178,\n",
       " 'might': 179,\n",
       " 'football': 180,\n",
       " 'bring': 181,\n",
       " 'keep': 182,\n",
       " 'sat': 183,\n",
       " 'trump': 184,\n",
       " 'shit': 185,\n",
       " '6th': 186,\n",
       " 'god': 187,\n",
       " 'join': 188,\n",
       " 'everyone': 189,\n",
       " 'brother': 190,\n",
       " 'via': 191,\n",
       " 'fight': 192,\n",
       " 'war': 193,\n",
       " 'use': 194,\n",
       " 'hot': 195,\n",
       " '2': 196,\n",
       " 'saw': 197,\n",
       " 'head': 198,\n",
       " 'justin': 199,\n",
       " 'ask': 200,\n",
       " 'little': 201,\n",
       " 'another': 202,\n",
       " 'oh': 203,\n",
       " 'obama': 204,\n",
       " 'state': 205,\n",
       " 'plan': 206,\n",
       " '7th': 207,\n",
       " 'put': 208,\n",
       " '1': 209,\n",
       " 'long': 210,\n",
       " 'always': 211,\n",
       " 'muslim': 212,\n",
       " 'cena': 213,\n",
       " '8th': 214,\n",
       " 'house': 215,\n",
       " 'already': 216,\n",
       " 'event': 217,\n",
       " 'nov': 218,\n",
       " 'band': 219,\n",
       " 'music': 220,\n",
       " 'christian': 221,\n",
       " 'book': 222,\n",
       " 'face': 223,\n",
       " 'round': 224,\n",
       " 'old': 225,\n",
       " 'follow': 226,\n",
       " 'stop': 227,\n",
       " 'amon': 228,\n",
       " 'move': 229,\n",
       " 'wanna': 230,\n",
       " 'tweet': 231,\n",
       " 'city': 232,\n",
       " 'happen': 233,\n",
       " 'match': 234,\n",
       " 'read': 235,\n",
       " 'december': 236,\n",
       " 'thanks': 237,\n",
       " 'town': 238,\n",
       " 'someone': 239,\n",
       " 'park': 240,\n",
       " 'price': 241,\n",
       " 'murray': 242,\n",
       " 'n': 243,\n",
       " 'help': 244,\n",
       " 'player': 245,\n",
       " 'gay': 246,\n",
       " 'yet': 247,\n",
       " 'class': 248,\n",
       " 'wish': 249,\n",
       " 'amazon': 250,\n",
       " 'support': 251,\n",
       " 'boy': 252,\n",
       " 'nicki': 253,\n",
       " 'drop': 254,\n",
       " 'lot': 255,\n",
       " 'kanye': 256,\n",
       " 'president': 257,\n",
       " 'something': 258,\n",
       " 'grade': 259,\n",
       " 'woman': 260,\n",
       " 'lady': 261,\n",
       " 'final': 262,\n",
       " 'jason': 263,\n",
       " 'away': 264,\n",
       " 'though': 265,\n",
       " 'yes': 266,\n",
       " 'finish': 267,\n",
       " 'dog': 268,\n",
       " '9th': 269,\n",
       " 'enjoy': 270,\n",
       " 'family': 271,\n",
       " 'tv': 272,\n",
       " 'sign': 273,\n",
       " 'half': 274,\n",
       " 'smith': 275,\n",
       " 'kill': 276,\n",
       " 'sam': 277,\n",
       " 'johnson': 278,\n",
       " 'thank': 279,\n",
       " 'point': 280,\n",
       " 'eid': 281,\n",
       " 'episode': 282,\n",
       " 'april': 283,\n",
       " 'fighter': 284,\n",
       " 'rollins': 285,\n",
       " 'fun': 286,\n",
       " 'many': 287,\n",
       " 'name': 288,\n",
       " 'zayn': 289,\n",
       " '3': 290,\n",
       " 'brown': 291,\n",
       " 'turn': 292,\n",
       " 'around': 293,\n",
       " 'google': 294,\n",
       " '10th': 295,\n",
       " 'paper': 296,\n",
       " 'oct': 297,\n",
       " 'deal': 298,\n",
       " 'stay': 299,\n",
       " 'islam': 300,\n",
       " 'wo': 301,\n",
       " 'part': 302,\n",
       " 'niall': 303,\n",
       " 'film': 304,\n",
       " 'kid': 305,\n",
       " 'seth': 306,\n",
       " 'actually': 307,\n",
       " 'thor': 308,\n",
       " 'jackson': 309,\n",
       " 'ed': 310,\n",
       " 'sept': 311,\n",
       " 'shawn': 312,\n",
       " 'return': 313,\n",
       " 'hey': 314,\n",
       " 'post': 315,\n",
       " 'believe': 316,\n",
       " 'kane': 317,\n",
       " 'sharknado': 318,\n",
       " 'pick': 319,\n",
       " 'ocean': 320,\n",
       " 'change': 321,\n",
       " 'forget': 322,\n",
       " 'madrid': 323,\n",
       " 'pretty': 324,\n",
       " 'israel': 325,\n",
       " 'date': 326,\n",
       " 'naruto': 327,\n",
       " 'rousey': 328,\n",
       " 'celebrate': 329,\n",
       " 'vote': 330,\n",
       " 'taylor': 331,\n",
       " 'ps4': 332,\n",
       " 'cube': 333,\n",
       " 'dunkin': 334,\n",
       " 'score': 335,\n",
       " 'iran': 336,\n",
       " 'foo': 337,\n",
       " 'excite': 338,\n",
       " 'club': 339,\n",
       " 'ant-man': 340,\n",
       " 'gucci': 341,\n",
       " 'brady': 342,\n",
       " 'perry': 343,\n",
       " 'early': 344,\n",
       " 'hold': 345,\n",
       " 'close': 346,\n",
       " 'eagle': 347,\n",
       " 'twilight': 348,\n",
       " 'b': 349,\n",
       " 'wear': 350,\n",
       " 'announce': 351,\n",
       " 'true': 352,\n",
       " 'nirvana': 353,\n",
       " 'g': 354,\n",
       " 'valentine': 355,\n",
       " 'dustin': 356,\n",
       " 'walk': 357,\n",
       " 'award': 358,\n",
       " 'yakub': 359,\n",
       " 'chance': 360,\n",
       " 'tour': 361,\n",
       " 'lesnar': 362,\n",
       " 'center': 363,\n",
       " 'brock': 364,\n",
       " 'dead': 365,\n",
       " 'prime': 366,\n",
       " 'till': 367,\n",
       " 'prince': 368,\n",
       " 'cause': 369,\n",
       " 'maybe': 370,\n",
       " 'month': 371,\n",
       " 'record': 372,\n",
       " 'nice': 373,\n",
       " 'wright': 374,\n",
       " 'forward': 375,\n",
       " 'probably': 376,\n",
       " 'write': 377,\n",
       " 'league': 378,\n",
       " 'finally': 379,\n",
       " 'sheeran': 380,\n",
       " 'line': 381,\n",
       " 'die': 382,\n",
       " 'michael': 383,\n",
       " 'must': 384,\n",
       " 'soul': 385,\n",
       " 'guess': 386,\n",
       " 'bieber': 387,\n",
       " 'kris': 388,\n",
       " 'sound': 389,\n",
       " 'rock': 390,\n",
       " 'roll': 391,\n",
       " 'dunne': 392,\n",
       " 'ok': 393,\n",
       " 'june': 394,\n",
       " 'pay': 395,\n",
       " '.': 396,\n",
       " 'curtis': 397,\n",
       " 'february': 398,\n",
       " 'x': 399,\n",
       " 'rest': 400,\n",
       " 'raw': 401,\n",
       " 'yeah': 402,\n",
       " 'st': 403,\n",
       " 'bryant': 404,\n",
       " 'baby': 405,\n",
       " 'late': 406,\n",
       " 'nothing': 407,\n",
       " 'gifford': 408,\n",
       " 'soon': 409,\n",
       " '14th': 410,\n",
       " 'blood': 411,\n",
       " 'job': 412,\n",
       " 'fact': 413,\n",
       " 'scott': 414,\n",
       " 'aldean': 415,\n",
       " '5': 416,\n",
       " 'rise': 417,\n",
       " 'wwe': 418,\n",
       " 'cream': 419,\n",
       " 'hogan': 420,\n",
       " 'iron': 421,\n",
       " 'minute': 422,\n",
       " 'sell': 423,\n",
       " 'debate': 424,\n",
       " 'become': 425,\n",
       " ':d': 426,\n",
       " 'catch': 427,\n",
       " 'walker': 428,\n",
       " 'gandhi': 429,\n",
       " 'yesterday': 430,\n",
       " 'harry': 431,\n",
       " 'jenner': 432,\n",
       " 'phone': 433,\n",
       " 'charlie': 434,\n",
       " 'seem': 435,\n",
       " 'ira': 436,\n",
       " 'special': 437,\n",
       " 'sleep': 438,\n",
       " 'awesome': 439,\n",
       " 'hulk': 440,\n",
       " '4': 441,\n",
       " 'excited': 442,\n",
       " 'photo': 443,\n",
       " 'kim': 444,\n",
       " 'jan': 445,\n",
       " 'church': 446,\n",
       " 'full': 447,\n",
       " 'country': 448,\n",
       " 'dark': 449,\n",
       " 'chelsea': 450,\n",
       " 'goal': 451,\n",
       " 'story': 452,\n",
       " 'cry': 453,\n",
       " 'mom': 454,\n",
       " 'haha': 455,\n",
       " 'usa': 456,\n",
       " 'london': 457,\n",
       " 'stone': 458,\n",
       " 'hate': 459,\n",
       " 'sad': 460,\n",
       " 'arsenal': 461,\n",
       " 'whole': 462,\n",
       " 'perform': 463,\n",
       " 'second': 464,\n",
       " 'blue': 465,\n",
       " 'title': 466,\n",
       " 'hand': 467,\n",
       " 'r': 468,\n",
       " 'person': 469,\n",
       " 'hard': 470,\n",
       " 'maiden': 471,\n",
       " 'mccartney': 472,\n",
       " 'fall': 473,\n",
       " 'far': 474,\n",
       " 'spend': 475,\n",
       " 'yoga': 476,\n",
       " 'due': 477,\n",
       " 'donald': 478,\n",
       " 'uk': 479,\n",
       " 'word': 480,\n",
       " 'kasich': 481,\n",
       " 'la': 482,\n",
       " 'cool': 483,\n",
       " 'bit': 484,\n",
       " 'question': 485,\n",
       " 'coach': 486,\n",
       " 'care': 487,\n",
       " 'saudi': 488,\n",
       " 'jurassic': 489,\n",
       " 'wonder': 490,\n",
       " 'single': 491,\n",
       " '10': 492,\n",
       " 'drive': 493,\n",
       " 'evans': 494,\n",
       " 'kick': 495,\n",
       " 'reason': 496,\n",
       " 'death': 497,\n",
       " 'act': 498,\n",
       " 'everything': 499,\n",
       " 'ball': 500,\n",
       " 'halloween': 501,\n",
       " 'tiger': 502,\n",
       " 'without': 503,\n",
       " 'note': 504,\n",
       " 'joe': 505,\n",
       " '<3': 506,\n",
       " 'race': 507,\n",
       " 'least': 508,\n",
       " 'york': 509,\n",
       " '‚òπ': 510,\n",
       " 'save': 511,\n",
       " 'dec': 512,\n",
       " 'rahul': 513,\n",
       " '6': 514,\n",
       " 'cover': 515,\n",
       " 'fiorina': 516,\n",
       " 'anniversary': 517,\n",
       " 'nfl': 518,\n",
       " 'straight': 519,\n",
       " 'case': 520,\n",
       " 'carly': 521,\n",
       " 'wrong': 522,\n",
       " 'dance': 523,\n",
       " 'bc': 524,\n",
       " 'dream': 525,\n",
       " 'ca': 526,\n",
       " 'interview': 527,\n",
       " '11th': 528,\n",
       " 'beach': 529,\n",
       " 'attack': 530,\n",
       " 'lamar': 531,\n",
       " 'caitlyn': 532,\n",
       " 'light': 533,\n",
       " 'iphone': 534,\n",
       " 'haram': 535,\n",
       " 'sorry': 536,\n",
       " 'luck': 537,\n",
       " 'wake': 538,\n",
       " 'christmas': 539,\n",
       " 'history': 540,\n",
       " 'sing': 541,\n",
       " 'update': 542,\n",
       " 'speak': 543,\n",
       " 'visit': 544,\n",
       " 'boko': 545,\n",
       " '13th': 546,\n",
       " 'else': 547,\n",
       " 'stage': 548,\n",
       " '20th': 549,\n",
       " 'money': 550,\n",
       " 'sale': 551,\n",
       " 'behind': 552,\n",
       " 'cup': 553,\n",
       " 'swift': 554,\n",
       " 'mayweather': 555,\n",
       " 'gotta': 556,\n",
       " 'king': 557,\n",
       " 'gaga': 558,\n",
       " 'hebdo': 559,\n",
       " 'order': 560,\n",
       " 'ipad': 561,\n",
       " 'briana': 562,\n",
       " 'enough': 563,\n",
       " 'floyd': 564,\n",
       " 'available': 565,\n",
       " 'fair': 566,\n",
       " 'aug': 567,\n",
       " 'son': 568,\n",
       " 'picture': 569,\n",
       " 'field': 570,\n",
       " 'festival': 571,\n",
       " 'wow': 572,\n",
       " 'stand': 573,\n",
       " 'high': 574,\n",
       " 'bowie': 575,\n",
       " 'hang': 576,\n",
       " 'send': 577,\n",
       " 'hi': 578,\n",
       " 'hell': 579,\n",
       " 'p': 580,\n",
       " 'mark': 581,\n",
       " 'arabia': 582,\n",
       " 'series': 583,\n",
       " 'room': 584,\n",
       " 'throw': 585,\n",
       " '12th': 586,\n",
       " 'dc': 587,\n",
       " '16th': 588,\n",
       " 'america': 589,\n",
       " 'teen': 590,\n",
       " 'twitter': 591,\n",
       " 'galaxy': 592,\n",
       " 'tony': 593,\n",
       " 'begin': 594,\n",
       " 'james': 595,\n",
       " 'side': 596,\n",
       " 'bless': 597,\n",
       " 'tie': 598,\n",
       " 'list': 599,\n",
       " 'force': 600,\n",
       " 'almost': 601,\n",
       " 'fire': 602,\n",
       " 'dont': 603,\n",
       " 'marriage': 604,\n",
       " 'agree': 605,\n",
       " 'allah': 606,\n",
       " 'moto': 607,\n",
       " 'share': 608,\n",
       " 'bed': 609,\n",
       " 'air': 610,\n",
       " '18th': 611,\n",
       " '25th': 612,\n",
       " 'mind': 613,\n",
       " 'netflix': 614,\n",
       " 'expect': 615,\n",
       " 'issue': 616,\n",
       " 'host': 617,\n",
       " 'wed': 618,\n",
       " 'rick': 619,\n",
       " 'together': 620,\n",
       " 'okay': 621,\n",
       " 'beautiful': 622,\n",
       " 'green': 623,\n",
       " 'report': 624,\n",
       " 'u2': 625,\n",
       " 'stuff': 626,\n",
       " 'launch': 627,\n",
       " 'interested': 628,\n",
       " 'spot': 629,\n",
       " 'hop': 630,\n",
       " 'learn': 631,\n",
       " 'american': 632,\n",
       " '15th': 633,\n",
       " 'medium': 634,\n",
       " 'pic': 635,\n",
       " 'favorite': 636,\n",
       " 'gen': 637,\n",
       " 'anything': 638,\n",
       " 'laugh': 639,\n",
       " 'perfect': 640,\n",
       " 'window': 641,\n",
       " 'heart': 642,\n",
       " 'instead': 643,\n",
       " 'mike': 644,\n",
       " 'eat': 645,\n",
       " 'car': 646,\n",
       " 'de': 647,\n",
       " '17th': 648,\n",
       " 'moment': 649,\n",
       " 'afternoon': 650,\n",
       " 'bush': 651,\n",
       " 'holiday': 652,\n",
       " 'continue': 653,\n",
       " 'amazing': 654,\n",
       " 'summer': 655,\n",
       " 'kpop': 656,\n",
       " 'trip': 657,\n",
       " 'sting': 658,\n",
       " 'parade': 659,\n",
       " 'shot': 660,\n",
       " 'omg': 661,\n",
       " 'able': 662,\n",
       " 'inning': 663,\n",
       " 'worst': 664,\n",
       " 'damn': 665,\n",
       " 'child': 666,\n",
       " ':)': 667,\n",
       " 'bill': 668,\n",
       " 'bbc': 669,\n",
       " 'katy': 670,\n",
       " 'member': 671,\n",
       " 'ur': 672,\n",
       " 'dad': 673,\n",
       " 'xbox': 674,\n",
       " 'along': 675,\n",
       " 'cole': 676,\n",
       " 'college': 677,\n",
       " 'labor': 678,\n",
       " 'josh': 679,\n",
       " 'major': 680,\n",
       " 'answer': 681,\n",
       " 'j': 682,\n",
       " 'test': 683,\n",
       " 'hopefully': 684,\n",
       " 'card': 685,\n",
       " 'eye': 686,\n",
       " 'ryan': 687,\n",
       " 'number': 688,\n",
       " 'peace': 689,\n",
       " 'zac': 690,\n",
       " 'harper': 691,\n",
       " 'young': 692,\n",
       " 'idea': 693,\n",
       " 'ted': 694,\n",
       " 'amy': 695,\n",
       " 'link': 696,\n",
       " 'decide': 697,\n",
       " 'practice': 698,\n",
       " 'shirt': 699,\n",
       " 'amendment': 700,\n",
       " 'pope': 701,\n",
       " 'welcome': 702,\n",
       " 'üòâ': 703,\n",
       " 'fly': 704,\n",
       " 'bear': 705,\n",
       " 'election': 706,\n",
       " '19th': 707,\n",
       " 'power': 708,\n",
       " '21st': 709,\n",
       " 'oliseh': 710,\n",
       " 'wood': 711,\n",
       " 'leader': 712,\n",
       " 'tho': 713,\n",
       " 'service': 714,\n",
       " 'add': 715,\n",
       " 'ago': 716,\n",
       " 'international': 717,\n",
       " 'group': 718,\n",
       " 'beyonce': 719,\n",
       " 'sport': 720,\n",
       " 'dallas': 721,\n",
       " 'radio': 722,\n",
       " 'hillary': 723,\n",
       " 'louis': 724,\n",
       " 'ahead': 725,\n",
       " 'bitch': 726,\n",
       " 'bob': 727,\n",
       " 'bowl': 728,\n",
       " 'pass': 729,\n",
       " 'le': 730,\n",
       " 'later': 731,\n",
       " 'food': 732,\n",
       " 'tory': 733,\n",
       " 'magic': 734,\n",
       " 'realize': 735,\n",
       " 'batman': 736,\n",
       " 'blair': 737,\n",
       " 'championship': 738,\n",
       " 'janet': 739,\n",
       " 'front': 740,\n",
       " 'surprise': 741,\n",
       " 'poll': 742,\n",
       " 'pride': 743,\n",
       " '30th': 744,\n",
       " 'three': 745,\n",
       " 'problem': 746,\n",
       " 'store': 747,\n",
       " 'funny': 748,\n",
       " 'san': 749,\n",
       " 'royal': 750,\n",
       " 'trend': 751,\n",
       " 'base': 752,\n",
       " 'sarah': 753,\n",
       " 'stream': 754,\n",
       " 'kardashian': 755,\n",
       " 'future': 756,\n",
       " 'randy': 757,\n",
       " 'osborne': 758,\n",
       " 'choice': 759,\n",
       " 'madonna': 760,\n",
       " 'law': 761,\n",
       " 'past': 762,\n",
       " 'greatest': 763,\n",
       " 'crazy': 764,\n",
       " 'feb': 765,\n",
       " 'sony': 766,\n",
       " 'barca': 767,\n",
       " 'undertaker': 768,\n",
       " 'plus': 769,\n",
       " 'tune': 770,\n",
       " 'huge': 771,\n",
       " 'glad': 772,\n",
       " 'grand': 773,\n",
       " 'bobby': 774,\n",
       " 'hall': 775,\n",
       " 'republican': 776,\n",
       " 'mr': 777,\n",
       " 'dude': 778,\n",
       " 'mac': 779,\n",
       " 'present': 780,\n",
       " 'ac': 781,\n",
       " 'hamilton': 782,\n",
       " 'cameron': 783,\n",
       " 'fri': 784,\n",
       " 'ipod': 785,\n",
       " 'jay': 786,\n",
       " 'confirm': 787,\n",
       " 'literally': 788,\n",
       " 'messi': 789,\n",
       " 'definitely': 790,\n",
       " 'dinner': 791,\n",
       " 'schedule': 792,\n",
       " 'row': 793,\n",
       " '15': 794,\n",
       " 'cant': 795,\n",
       " 'art': 796,\n",
       " 'bet': 797,\n",
       " '12': 798,\n",
       " 'appear': 799,\n",
       " 'heat': 800,\n",
       " 'proud': 801,\n",
       " 'kerry': 802,\n",
       " 'jeb': 803,\n",
       " 'orton': 804,\n",
       " 'either': 805,\n",
       " 'derby': 806,\n",
       " 'annual': 807,\n",
       " 'pull': 808,\n",
       " 'wolf': 809,\n",
       " 'kurt': 810,\n",
       " 'giant': 811,\n",
       " 'united': 812,\n",
       " 'lie': 813,\n",
       " 'charge': 814,\n",
       " 'e': 815,\n",
       " 'cut': 816,\n",
       " 'pitch': 817,\n",
       " 'include': 818,\n",
       " 'nokia': 819,\n",
       " 'joke': 820,\n",
       " 'ya': 821,\n",
       " 'official': 822,\n",
       " 'double': 823,\n",
       " 'gop': 824,\n",
       " 'court': 825,\n",
       " 'review': 826,\n",
       " 'mon': 827,\n",
       " 'office': 828,\n",
       " 'h': 829,\n",
       " 'street': 830,\n",
       " 'biden': 831,\n",
       " 'extra': 832,\n",
       " 'men': 833,\n",
       " 'shoot': 834,\n",
       " 'coffee': 835,\n",
       " 'lmao': 836,\n",
       " 'dress': 837,\n",
       " 'career': 838,\n",
       " 'course': 839,\n",
       " 'anderson': 840,\n",
       " '30': 841,\n",
       " 'thought': 842,\n",
       " 'champion': 843,\n",
       " 'bernie': 844,\n",
       " 'watchman': 845,\n",
       " '7': 846,\n",
       " 'understand': 847,\n",
       " 'nigga': 848,\n",
       " 'offer': 849,\n",
       " 'mcgregor': 850,\n",
       " 'cruise': 851,\n",
       " 'martin': 852,\n",
       " 'attend': 853,\n",
       " 'evening': 854,\n",
       " 'performance': 855,\n",
       " 'mad': 856,\n",
       " 'eric': 857,\n",
       " 'miley': 858,\n",
       " 'throne': 859,\n",
       " 'serena': 860,\n",
       " 'train': 861,\n",
       " 'hr': 862,\n",
       " 'stadium': 863,\n",
       " 'smile': 864,\n",
       " 'daily': 865,\n",
       " '23rd': 866,\n",
       " 'dr': 867,\n",
       " 'campaign': 868,\n",
       " 'juventus': 869,\n",
       " 'wife': 870,\n",
       " 'cancel': 871,\n",
       " 'winner': 872,\n",
       " 'williams': 873,\n",
       " 'worth': 874,\n",
       " 'beckham': 875,\n",
       " 'lexus': 876,\n",
       " '20': 877,\n",
       " 'drink': 878,\n",
       " 'road': 879,\n",
       " 'result': 880,\n",
       " 'jack': 881,\n",
       " 'io': 882,\n",
       " 'sister': 883,\n",
       " 'sep': 884,\n",
       " 'federer': 885,\n",
       " 'jindal': 886,\n",
       " 'til': 887,\n",
       " '24th': 888,\n",
       " 'business': 889,\n",
       " 'kind': 890,\n",
       " 'market': 891,\n",
       " 'action': 892,\n",
       " 'amaze': 893,\n",
       " 'gov': 894,\n",
       " 'schumer': 895,\n",
       " 'million': 896,\n",
       " 'loss': 897,\n",
       " 'suck': 898,\n",
       " '):': 899,\n",
       " 'potter': 900,\n",
       " 'sander': 901,\n",
       " 'main': 902,\n",
       " 'nike': 903,\n",
       " 'grateful': 904,\n",
       " 'merkel': 905,\n",
       " 'flair': 906,\n",
       " 'voice': 907,\n",
       " 'ben': 908,\n",
       " 'biggest': 909,\n",
       " 'thurs': 910,\n",
       " 'dana': 911,\n",
       " 'chuck': 912,\n",
       " 'rule': 913,\n",
       " 'ric': 914,\n",
       " 'snoop': 915,\n",
       " 'cobain': 916,\n",
       " 'short': 917,\n",
       " 'consider': 918,\n",
       " 'jam': 919,\n",
       " 'view': 920,\n",
       " 'scotus': 921,\n",
       " 'washington': 922,\n",
       " \"y'all\": 923,\n",
       " 'edge': 924,\n",
       " 'different': 925,\n",
       " 'yr': 926,\n",
       " 'mention': 927,\n",
       " '2012': 928,\n",
       " 'style': 929,\n",
       " 'four': 930,\n",
       " 'nintendo': 931,\n",
       " 'ukip': 932,\n",
       " 'minecraft': 933,\n",
       " 'hotel': 934,\n",
       " 'moon': 935,\n",
       " 'serve': 936,\n",
       " 'italy': 937,\n",
       " 'venice': 938,\n",
       " 'braun': 939,\n",
       " 'vs': 940,\n",
       " 'paris': 941,\n",
       " 'liam': 942,\n",
       " 'palin': 943,\n",
       " 'strong': 944,\n",
       " 'matter': 945,\n",
       " 'lee': 946,\n",
       " 'michelle': 947,\n",
       " 'tsipras': 948,\n",
       " 'age': 949,\n",
       " 'hat': 950,\n",
       " 'meeting': 951,\n",
       " 'apparently': 952,\n",
       " 'third': 953,\n",
       " 'zlatan': 954,\n",
       " 'remind': 955,\n",
       " 'others': 956,\n",
       " 'download': 957,\n",
       " 'mar': 958,\n",
       " 'k': 959,\n",
       " 'dogg': 960,\n",
       " 'mariah': 961,\n",
       " 'couple': 962,\n",
       " 'british': 963,\n",
       " 'seriously': 964,\n",
       " 'marley': 965,\n",
       " 'conor': 966,\n",
       " 'celebrity': 967,\n",
       " 'oracle': 968,\n",
       " 'parenthood': 969,\n",
       " 'near': 970,\n",
       " 'ai': 971,\n",
       " 'theme': 972,\n",
       " 'piece': 973,\n",
       " 'info': 974,\n",
       " 'receive': 975,\n",
       " 'track': 976,\n",
       " 'treat': 977,\n",
       " 'disneyland': 978,\n",
       " 'bar': 979,\n",
       " 'angela': 980,\n",
       " 't-mobile': 981,\n",
       " 'erdogan': 982,\n",
       " 'sweet': 983,\n",
       " 'email': 984,\n",
       " 'debut': 985,\n",
       " 'channel': 986,\n",
       " 'quarter': 987,\n",
       " 'east': 988,\n",
       " 'worry': 989,\n",
       " 'arrive': 990,\n",
       " 'bee': 991,\n",
       " 'metlife': 992,\n",
       " 'hannibal': 993,\n",
       " 'jay-z': 994,\n",
       " 'ibm': 995,\n",
       " 'fine': 996,\n",
       " 'online': 997,\n",
       " 'classic': 998,\n",
       " 'fail': 999,\n",
       " 'india': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters=' ')\n",
    "tokenizer.fit_on_texts(all_tweet)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: \"win match plus tomorrow busy day awareness day debate gulp debate ...\" -> [43, 234, 769, 2, 1054, 6, 2896, 6, 424, 12301, 424, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train_3 = tokenizer.texts_to_sequences(tweets_train_3)\n",
    "sequences_train_7 = tokenizer.texts_to_sequences(tweets_train_7)\n",
    "\n",
    "sequences = sequences_train_3 + sequences_train_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the max tweet sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 0\n",
    "for elt in sequences:\n",
    "    if len(elt) > MAX_SEQUENCE_LENGTH:\n",
    "        MAX_SEQUENCE_LENGTH = len(elt)\n",
    "\n",
    "print(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37785, 300)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the embedded matrix with weight from embedding file or from saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_load():\n",
    "    try:\n",
    "        word2vec = KeyedVectors.load(\"data/word2vect.w2v\")\n",
    "        print(\"a\")\n",
    "        return word2vec\n",
    "    except:\n",
    "        word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "        print(\"b\")\n",
    "        return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2vect = word2vec_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the oov randomly (oov: out of vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov = []\n",
    "oov.append((np.random.rand(EMBEDDING_DIM) * 2.0) - 1.0) # uniform distribution on [-1, 1]\n",
    "oov = oov / np.linalg.norm(oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the matrix with the word_embedding and oov if the word doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e1646ffc7891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moov\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word2vec' is not defined"
     ]
    }
   ],
   "source": [
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "    else:\n",
    "        embedding_matrix[i] = oov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_3 = pad_sequences(sequences_train_3, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_train_7 = pad_sequences(sequences_train_7, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_train_3 = np.arange(data_train_3.shape[0])\n",
    "#data_train_3 = data_train_3[indices_train_3]\n",
    "\n",
    "labels_train_3 = to_categorical(np.asarray(sentiments_train_3), 3)\n",
    "#labels_train_3 = labels_train_3[indices_train_3]\n",
    "\n",
    "labels_train_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_train_7 = np.arange(data_train_7.shape[0])\n",
    "#data_train_7 = data_train_7[indices_train_7]\n",
    "\n",
    "labels_train_7 = to_categorical(np.asarray(sentiments_train_7), 7)\n",
    "#labels_train_7 = labels_train_7[indices_train_7]\n",
    "\n",
    "labels_train_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 35233 samples\n",
      "validation set: 15100 samples\n",
      "x_train_3 shape:  (35233, 32)\n",
      "y_train_3 shape:  (35233, 3)\n"
     ]
    }
   ],
   "source": [
    "split_idx = int(len(data_train_3) * 0.70)\n",
    "\n",
    "x_train_3, x_val_3 = data_train_3[:split_idx], data_train_3[split_idx:]\n",
    "y_train_3, y_val_3 = labels_train_3 [:split_idx], labels_train_3[split_idx:]\n",
    "\n",
    "print('training set: ' + str(len(x_train_3)) + ' samples')\n",
    "print('validation set: ' + str(len(x_val_3)) + ' samples')\n",
    "# print('test set: ' + str(len(x_test)) + ' samples')\n",
    "\n",
    "print('x_train_3 shape: ', x_train_3.shape)\n",
    "print('y_train_3 shape: ', y_train_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 1385 samples\n",
      "validation set: 245 samples\n",
      "x_train_7 shape:  (1385, 32)\n",
      "y_train_7 shape:  (1385, 7)\n"
     ]
    }
   ],
   "source": [
    "split_idx = int(len(data_train_7) * 0.85)\n",
    "\n",
    "x_train_7, x_val_7 = data_train_7[:split_idx], data_train_7[split_idx:]\n",
    "y_train_7, y_val_7 = labels_train_7 [:split_idx], labels_train_7[split_idx:]\n",
    "\n",
    "print('training set: ' + str(len(x_train_7)) + ' samples')\n",
    "print('validation set: ' + str(len(x_val_7)) + ' samples')\n",
    "\n",
    "print('x_train_7 shape: ', x_train_7.shape)\n",
    "print('y_train_7 shape: ', y_train_7.shape)\n",
    "\n",
    "#, y_train_7, x_val_7,y_train_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False, name='embedding_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(x_train_3, y_train_3, x_val_3, y_val_3, embedding_layer):\n",
    "\n",
    "    model1 = Sequential()\n",
    "    \n",
    "    model1.add(embedding_layer)\n",
    "    model1.add(LSTM(32))\n",
    "    model1.add(Dropout(0.2))\n",
    "    model1.add(Dense(32, activation='relu'))\n",
    "    model1.add(Dropout(0.2))\n",
    "    model1.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    model1.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "    model1.summary()\n",
    "    history = model1.fit(x_train_3, y_train_3, validation_data=(x_val_3, y_val_3), epochs=6, batch_size=50)\n",
    "    \n",
    "    model1.save(\"./model1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(x_train_3, y_train_3, x_val_3, y_val_3, embedding_layer, epochs, batch_size):\n",
    "\n",
    "    model2 = Sequential()\n",
    "    \n",
    "    model2.add(embedding_layer)\n",
    "    model2.add(GRU(32))\n",
    "    model2.add(Dropout(0.2))\n",
    "    model2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model2.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "    model2.summary()\n",
    "    history = model2.fit(x_train_3, y_train_3, validation_data=(x_val_3, y_val_3), epochs=6, batch_size=50)\n",
    "\n",
    "    model2.save(\"./model2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 32, 300)           11335500  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 11,379,279\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 11,335,500\n",
      "_________________________________________________________________\n",
      "Train on 35233 samples, validate on 15100 samples\n",
      "Epoch 1/6\n",
      "35233/35233 [==============================] - 18s 498us/step - loss: 1.0395 - acc: 0.4657 - val_loss: 1.0341 - val_acc: 0.4085\n",
      "Epoch 2/6\n",
      "35233/35233 [==============================] - 16s 456us/step - loss: 1.0106 - acc: 0.4661 - val_loss: 1.0344 - val_acc: 0.4085\n",
      "Epoch 3/6\n",
      "35233/35233 [==============================] - 16s 459us/step - loss: 1.0092 - acc: 0.4661 - val_loss: 1.0342 - val_acc: 0.4085\n",
      "Epoch 4/6\n",
      "35233/35233 [==============================] - 16s 464us/step - loss: 1.0092 - acc: 0.4661 - val_loss: 1.0334 - val_acc: 0.4085\n",
      "Epoch 5/6\n",
      "35233/35233 [==============================] - 16s 459us/step - loss: 1.0092 - acc: 0.4661 - val_loss: 1.0344 - val_acc: 0.4085\n",
      "Epoch 6/6\n",
      "35233/35233 [==============================] - 16s 465us/step - loss: 1.0092 - acc: 0.4661 - val_loss: 1.0340 - val_acc: 0.4085\n"
     ]
    }
   ],
   "source": [
    "model1(x_train_3, y_train_3, x_val_3, y_val_3, embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfert learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 32, 300)           11335500  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 11,379,279\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 11,335,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"./model1.h5\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut 2 last layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x7f2395a61438>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "#model.outputs = [model.layers[-1].output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 32, 300)           11335500  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 150)               600       \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 64)                9664      \n",
      "_________________________________________________________________\n",
      "dense3 (Dense)               (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 11,379,279\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 11,335,500\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dadou/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1385 samples, validate on 245 samples\n",
      "Epoch 1/11\n",
      "1385/1385 [==============================] - 2s 1ms/step - loss: 1.8802 - acc: 0.2664 - val_loss: 1.8654 - val_acc: 0.2327\n",
      "Epoch 2/11\n",
      "1385/1385 [==============================] - 1s 445us/step - loss: 1.8380 - acc: 0.2809 - val_loss: 1.8619 - val_acc: 0.2327\n",
      "Epoch 3/11\n",
      "1385/1385 [==============================] - 1s 435us/step - loss: 1.8359 - acc: 0.2809 - val_loss: 1.8605 - val_acc: 0.2327\n",
      "Epoch 4/11\n",
      "1385/1385 [==============================] - 1s 430us/step - loss: 1.8368 - acc: 0.2809 - val_loss: 1.8593 - val_acc: 0.2327\n",
      "Epoch 5/11\n",
      "1385/1385 [==============================] - 1s 426us/step - loss: 1.8350 - acc: 0.2809 - val_loss: 1.8670 - val_acc: 0.2327\n",
      "Epoch 6/11\n",
      "1385/1385 [==============================] - 1s 420us/step - loss: 1.8360 - acc: 0.2809 - val_loss: 1.8614 - val_acc: 0.2327\n",
      "Epoch 7/11\n",
      "1385/1385 [==============================] - 1s 438us/step - loss: 1.8356 - acc: 0.2809 - val_loss: 1.8640 - val_acc: 0.2327\n",
      "Epoch 8/11\n",
      "1385/1385 [==============================] - 1s 427us/step - loss: 1.8363 - acc: 0.2809 - val_loss: 1.8639 - val_acc: 0.2327\n",
      "Epoch 9/11\n",
      "1385/1385 [==============================] - 1s 423us/step - loss: 1.8354 - acc: 0.2809 - val_loss: 1.8675 - val_acc: 0.2327\n",
      "Epoch 10/11\n",
      "1385/1385 [==============================] - 1s 430us/step - loss: 1.8356 - acc: 0.2809 - val_loss: 1.8645 - val_acc: 0.2327\n",
      "Epoch 11/11\n",
      "1385/1385 [==============================] - 1s 423us/step - loss: 1.8353 - acc: 0.2809 - val_loss: 1.8596 - val_acc: 0.2327\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(150, activation='relu', name='dense1'))\n",
    "model.add(Dense(64, activation='relu', name='dense2'))\n",
    "model.add(Dense(7, activation='softmax', name='dense3'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train_7, y_train_7, validation_data=(x_val_7, y_val_7), epochs=11, batch_size=50)\n",
    "model.save(\"./model3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing new transfert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/task_A/data_test_7.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c4306172c9da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets_test_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preprocessing_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpora_test_7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msequences_test_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_test_7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_test_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences_test_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test_7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Study/Epita/SCIA/S9/DNLP/Tweets-Sentiment-Analysis/TP_transfer_learning_2018/preprocessing.py\u001b[0m in \u001b[0;36mdata_preprocessing_test\u001b[0;34m(path_tweets)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_preprocessing_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstandardization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data/task_A/data_test_7.csv' does not exist"
     ]
    }
   ],
   "source": [
    "tweets_test_7 = data_preprocessing_test(corpora_test_7)\n",
    "sequences_test_7 = tokenizer.texts_to_sequences(tweets_test_7)\n",
    "data_test_7 = pad_sequences(sequences_test_7, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "r = model.predict(data_test_7)\n",
    "data = pd.read_csv(\"/home/abdou/Documents/datastories-semeval2017-task4-master/dataset/Subtask_A/gold/2018-Valence-oc-En-test-gold.txt\", sep='\\t', encoding='utf-8')\n",
    "\n",
    "for i in range(len(r)):\n",
    "    data['Intensity Class'][i]=[\"-1: slightly negative emotional state can be inferred\",\n",
    "                                \"-2: moderately negative emotional state can be inferred\",\n",
    "                                \"-3: very negative emotional state can be inferred\",\n",
    "                                \"0: neutral or mixed emotional state can be inferred\",\n",
    "                                \"1: slightly positive emotional state can be inferred\",\n",
    "                                \"2: moderately positive emotional state can be inferred\",\n",
    "                                \"3: very positive emotional state can be inferred\"][numpy.argmax(r[i])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write predicting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./submission_after.csv\", \"w\")\n",
    "f.write(\"ID\tTweet\tAffect Dimension\tIntensity Class\\n\")\n",
    "\n",
    "for d in range(len(data)):\n",
    "    f.write(data['ID'][d] + \"\\t\" + data['Tweet'][d] + \"\\tvalence\\t\" + data['Intensity Class'][d] + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
