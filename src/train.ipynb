{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <CENTER>Tweet sentiment analysis</CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- (Embedding tweet instead of google news)\n",
    "- Add OLE on embedding (+2 into dim)\n",
    "- Add EV on embedding (+11 into dim)\n",
    "- \n",
    "- Cross validation\n",
    "- Comment v√©rifier l'overfitting?\n",
    "- Jouer avec hyperparametres (dropout, fonction activation, nb de neuronnes, nb de layers, nb d'epoche / batch, optimisation, architecture, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Python core and 3rd-party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil # RAM monitoring\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import Dense, Dropout, GRU, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Our custom preprocessing module\n",
    "from preprocessing import data_preprocessing, data_preprocessing_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.layers import Bidirectional, Flatten, Input \n",
    "\n",
    "#import matplotlib as mpl\n",
    "#from preprocessing import standardization, data_preprocessing\n",
    "#mpl.use('TkAgg')  # or whatever other backend that you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7) # random state\n",
    "\n",
    "CORPUS_TRAIN_3 = \"data/tweet/data_train_3.csv\" # training part of the corpus for the model with 3 output classes\n",
    "CORPUS_TEST_3 = \"data/tweet/data_test_3.csv\" # testing part of the corpus for the model with 3 output classes\n",
    "CORPUS_TRAIN_7 = \"data/tweet/data_train_7.csv\" # training part of the corpus for the model with 7 output classes\n",
    "\n",
    "# FIXME: Currently, not available!\n",
    "CORPUS_TEST_7 = CORPUS_TEST_3  # testing part of the corpus for the model with 7 output classes\n",
    "#CORPUS_TEST_7 = \"data/task_A/data_test_7.csv\"\n",
    "\n",
    "EMBEDDING_W2C_FILE = \"data/embedding/GoogleNews-vectors-negative300.bin\" # Google word2vec's file (~3.5GB)\n",
    "EMBEDDING_MATRIX_FILE = \"data/embedding/embedding_matrix.csv\" # the embedding matrix exported to CSV (~200MB)\n",
    "BASE_EMBEDDING_DIM = 300 # Embedding (word2vec) dimension\n",
    "\n",
    "# FIXME\n",
    "EXTRA_EMBEDDING_DIM = 0 # Add extra dimensions for polarities, etc\n",
    "EMBEDDING_DIM = BASE_EMBEDDING_DIM + EXTRA_EMBEDDING_DIM # Total embedding dimensions\n",
    "\n",
    "# Required memory (in GB) to run this notebook smoothly\n",
    "REQUIRED_MEMORY = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "\"Won the match #getin . Plus\\u002c tomorrow is a very busy day\\u002c with Awareness Day\\u2019s and debates. Gulp. Debates...\" <BR>\n",
    "-> \"win match plus tomorrow busy day awareness day debate gulp debate ...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIXME** for \\*\\_train_7: data_preprocessing(CORPUS_TRAIN_7, **'train'**) **???**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_3, sentiments_train_3 = data_preprocessing(CORPUS_TRAIN_3, 'train')\n",
    "tweets_train_7, sentiments_train_7 = data_preprocessing(CORPUS_TRAIN_7, 'test')\n",
    "\n",
    "#FIXME \n",
    "#data = pd.read_csv(\"/home/abdou/Documents/datastories-semeval2017-task4-master/dataset/Subtask_A/gold/2018-Valence-oc-En-test-gold.txt\", sep='\\t', encoding='utf-8')\n",
    "#tweets_test_7 = data_preprocessing_test(data)\n",
    "tweets_test_7 = tweets_train_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the tweets and get the word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37784"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters=' ')\n",
    "all_tweets = tweets_train_3.append(tweets_train_7) # append both Pandas series\n",
    "tokenizer.fit_on_texts(all_tweets)\n",
    "word_index = tokenizer.word_index # a dictionary {word: index}\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: \"win match plus tomorrow busy day awareness day debate gulp debate ...\" -> [43, 234, 769, 2, 1054, 6, 2896, 6, 424, 12301, 424, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train_3 = tokenizer.texts_to_sequences(tweets_train_3)\n",
    "sequences_train_7 = tokenizer.texts_to_sequences(tweets_train_7)\n",
    "sequences_test_7 = tokenizer.texts_to_sequences(tweets_test_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the max tweet sequence and pad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = max(len(elt) for elt in sequences_train_3 + sequences_train_7)\n",
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_3 = pad_sequences(sequences_train_3, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_train_7 = pad_sequences(sequences_train_7, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_test_7 = pad_sequences(sequences_test_7, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=8241569792, available=3070816256, percent=62.7, used=4467982336, free=1106837504, active=4933869568, inactive=1625284608, buffers=398352384, cached=2268397568, shared=390672384, slab=400437248)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(generate_embedding_matrix=None):\n",
    "  # Decide if we generate the embedding matrix from from the Google word2vec file,\n",
    "  # our read it directly from a CSV file we created before\n",
    "  \n",
    "  if generate_embedding_matrix is None:\n",
    "    # check if there is enough RAM available\n",
    "    available_memory = psutil.virtual_memory().available\n",
    "    generate_embedding_matrix = (available_memory >= REQUIRED_MEMORY * 2**30)\n",
    "    \n",
    "    # check if the Google word2vec file can be opened successfully\n",
    "    try: \n",
    "      with open(EMBEDDING_W2C_FILE) as _: \n",
    "        pass\n",
    "    except:\n",
    "      print(\"Could not open the Google word2vec file\")\n",
    "      generate_embedding_matrix = False\n",
    "    \n",
    "  # Generate the embedding matrix using either method\n",
    "  if generate_embedding_matrix:\n",
    "    print(\"Generating the embedding matrix from the Google word2vec file, please wait...\")\n",
    "  \n",
    "    # We need 1 more line for oov (out of vocabulary words)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "  \n",
    "    # Load the Google word2vec file. ** This is the RAM-consuming step! **\n",
    "    word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_W2C_FILE, binary=True)\n",
    "  \n",
    "    # Fill the 300 first columns of the matrix from the Google word2vec file.\n",
    "    # Replace the oov with a random embedding vector.\n",
    "    oov = 2.0 * np.random.rand(BASE_EMBEDDING_DIM) - 1.0\n",
    "    oov /= np.linalg.norm(oov)\n",
    "    for word, i in word_index.items():\n",
    "      if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "      else:\n",
    "        embedding_matrix[i] = oov\n",
    "\n",
    "    # Export the result to CSV using Pandas for future use on a less powerful computer\n",
    "    pd.DataFrame(embedding_matrix).to_csv(EMBEDDING_MATRIX_FILE, header=False, index=False)\n",
    "    print(\"Done.\")\n",
    "        \n",
    "  else:\n",
    "    # The CSV file mentioned here was created before as above, but on a more powerful computer\n",
    "    print(\"Reading the embedding matrix from a CSV file of us... Done.\") \n",
    "\n",
    "    # Import the embedding matrix from CSV using Pandas\n",
    "    embedding_matrix = pd.read_csv(EMBEDDING_MATRIX_FILE, header=None)\n",
    "    \n",
    "  return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not open the Google word2vec file\n",
      "Reading the embedding matrix from a CSV file of us... Done.\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = get_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37785, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             0         1         2         3         4         5         6    \\\n",
       "0      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1     -0.084055  0.055533 -0.012219  0.044333  0.094827  0.007637  0.000222   \n",
       "2     -0.020020  0.011353  0.188477 -0.182617  0.047363  0.041016  0.017822   \n",
       "3      0.084961 -0.095215  0.119141  0.111816 -0.111328  0.049805  0.114258   \n",
       "4      0.059814  0.023804  0.067383  0.316406 -0.189453  0.016479  0.153320   \n",
       "5     -0.026367  0.068359 -0.031128  0.219727  0.003418 -0.009033  0.107910   \n",
       "6     -0.150391  0.072266  0.030518  0.041504  0.018066 -0.036865  0.168945   \n",
       "7      0.033203 -0.089844 -0.294922  0.115234 -0.071289 -0.053955  0.010498   \n",
       "8     -0.055664  0.008911 -0.092285  0.214844 -0.198242 -0.036133  0.211914   \n",
       "9      0.015747  0.076660 -0.032227  0.020142  0.015259 -0.052979 -0.014465   \n",
       "10    -0.291016 -0.026978  0.025757  0.109375 -0.018799 -0.241211 -0.168945   \n",
       "11    -0.047363  0.187500  0.002258  0.173828 -0.015991 -0.150391  0.113770   \n",
       "12     0.103516  0.137695 -0.002975  0.181641 -0.000243  0.106934  0.197266   \n",
       "13    -0.053223 -0.000412  0.033691  0.131836  0.063477 -0.024658 -0.070801   \n",
       "14     0.220703  0.068359  0.066895  0.149414 -0.055664 -0.162109 -0.355469   \n",
       "15     0.021851  0.060791  0.045410  0.126953 -0.125000 -0.052246 -0.020874   \n",
       "16    -0.113281 -0.036865  0.094238  0.007996  0.024902 -0.166992  0.036621   \n",
       "17     0.007812  0.020508  0.189453  0.285156 -0.255859  0.046143  0.023682   \n",
       "18    -0.099609  0.011047  0.006531  0.255859  0.168945 -0.132812 -0.033447   \n",
       "19     0.045654 -0.145508  0.156250  0.166016  0.109863  0.007507  0.073730   \n",
       "20    -0.253906  0.046631  0.164062 -0.018311 -0.283203 -0.126953 -0.213867   \n",
       "21    -0.036133 -0.121094  0.133789  0.114258 -0.172852  0.082031  0.218750   \n",
       "22     0.114746  0.052734  0.042969  0.004486  0.273438  0.071777  0.173828   \n",
       "23     0.259766  0.068848  0.063965  0.108398 -0.031738 -0.072266 -0.386719   \n",
       "24     0.011292  0.028931  0.083496 -0.049805 -0.130859 -0.109863 -0.115234   \n",
       "25     0.136719  0.148438  0.114746  0.069824 -0.166992 -0.006287  0.351562   \n",
       "26    -0.151367 -0.032471  0.142578  0.226562  0.066895  0.011536 -0.068848   \n",
       "27    -0.046875  0.066895  0.009338  0.263672 -0.012451 -0.039551  0.222656   \n",
       "28    -0.054932 -0.117188  0.027832  0.074707 -0.197266  0.093262  0.181641   \n",
       "29    -0.074707 -0.058838  0.045654 -0.129883 -0.008179 -0.035400  0.020630   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "37755 -0.084055  0.055533 -0.012219  0.044333  0.094827  0.007637  0.000222   \n",
       "37756 -0.070801 -0.195312  0.012390  0.011047  0.024902 -0.003540 -0.172852   \n",
       "37757  0.095703 -0.015991 -0.183594 -0.091309 -0.057861 -0.046875 -0.232422   \n",
       "37758 -0.084055  0.055533 -0.012219  0.044333  0.094827  0.007637  0.000222   \n",
       "37759 -0.084055  0.055533 -0.012219  0.044333  0.094827  0.007637  0.000222   \n",
       "37760  0.072266  0.111328  0.135742 -0.134766 -0.206055  0.090332 -0.130859   \n",
       "37761 -0.218750  0.212891 -0.078613  0.030518 -0.214844  0.075684 -0.253906   \n",
       "37762  0.012634  0.065430 -0.052979 -0.047607 -0.337891  0.337891  0.069336   \n",
       "37763  0.181641  0.065918  0.202148  0.310547  0.077637 -0.027466 -0.099121   \n",
       "37764 -0.084055  0.055533 -0.012219  0.044333  0.094827  0.007637  0.000222   \n",
       "37765 -0.084055  0.055533 -0.012219  0.044333  0.094827  0.007637  0.000222   \n",
       "37766 -0.010437  0.058350  0.050049  0.023193  0.136719  0.326172  0.298828   \n",
       "37767 -0.084055  0.055533 -0.012219  0.044333  0.094827  0.007637  0.000222   \n",
       "37768 -0.084055  0.055533 -0.012219  0.044333  0.094827  0.007637  0.000222   \n",
       "37769  0.108398  0.275391  0.072754 -0.115723  0.029541 -0.054199  0.206055   \n",
       "37770  0.021973 -0.107422 -0.057861  0.065430  0.131836 -0.078125 -0.033447   \n",
       "37771 -0.055176 -0.117676 -0.267578  0.239258 -0.045166  0.110352 -0.112793   \n",
       "37772 -0.084055  0.055533 -0.012219  0.044333  0.094827  0.007637  0.000222   \n",
       "37773  0.018799 -0.096191 -0.139648  0.283203 -0.153320  0.056885  0.066406   \n",
       "37774 -0.084055  0.055533 -0.012219  0.044333  0.094827  0.007637  0.000222   \n",
       "37775 -0.084055  0.055533 -0.012219  0.044333  0.094827  0.007637  0.000222   \n",
       "37776 -0.205078  0.061035 -0.084961 -0.136719  0.121094  0.101562  0.012024   \n",
       "37777 -0.084055  0.055533 -0.012219  0.044333  0.094827  0.007637  0.000222   \n",
       "37778  0.120117  0.064453 -0.224609  0.159180 -0.052490 -0.202148  0.093262   \n",
       "37779 -0.084055  0.055533 -0.012219  0.044333  0.094827  0.007637  0.000222   \n",
       "37780  0.033447  0.024780 -0.058594  0.175781 -0.062988 -0.127930  0.030762   \n",
       "37781  0.086914 -0.123535  0.049316  0.298828 -0.155273  0.116699  0.034424   \n",
       "37782 -0.084055  0.055533 -0.012219  0.044333  0.094827  0.007637  0.000222   \n",
       "37783 -0.084055  0.055533 -0.012219  0.044333  0.094827  0.007637  0.000222   \n",
       "37784 -0.135742 -0.005524 -0.162109  0.106934 -0.201172 -0.241211  0.140625   \n",
       "\n",
       "            7         8         9      ...          290       291       292  \\\n",
       "0      0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "1     -0.084900 -0.045939 -0.000023    ...    -0.069590 -0.067214 -0.088728   \n",
       "2      0.000200  0.088867  0.101074    ...     0.180664 -0.137695 -0.281250   \n",
       "3     -0.098633  0.099609 -0.041504    ...    -0.066406  0.034424 -0.005707   \n",
       "4      0.025269  0.107910 -0.017578    ...    -0.109863 -0.064453  0.053467   \n",
       "5     -0.174805  0.077148  0.000383    ...     0.041260  0.298828 -0.232422   \n",
       "6     -0.125000  0.218750  0.045898    ...     0.109863 -0.041992 -0.168945   \n",
       "7      0.119141  0.024536  0.080078    ...    -0.021729  0.182617 -0.086914   \n",
       "8     -0.085449  0.107422  0.144531    ...    -0.068359 -0.008667 -0.030396   \n",
       "9     -0.195312  0.058838  0.125000    ...     0.136719  0.027954 -0.250000   \n",
       "10    -0.125000  0.117188  0.056152    ...     0.273438  0.079102 -0.049561   \n",
       "11    -0.015503  0.219727 -0.010132    ...    -0.158203  0.031738 -0.179688   \n",
       "12     0.007507 -0.084473  0.136719    ...    -0.001320  0.042725 -0.037598   \n",
       "13    -0.204102  0.044678  0.217773    ...     0.458984  0.052002 -0.114258   \n",
       "14     0.078125  0.148438  0.039795    ...     0.163086 -0.106934 -0.250000   \n",
       "15     0.180664  0.078613  0.150391    ...    -0.168945  0.180664 -0.062988   \n",
       "16     0.073242  0.213867 -0.038574    ...    -0.127930  0.137695 -0.123535   \n",
       "17    -0.111328  0.150391  0.038330    ...     0.185547  0.068359 -0.097656   \n",
       "18    -0.197266  0.053223  0.166016    ...     0.175781  0.136719 -0.228516   \n",
       "19    -0.031006  0.157227  0.099609    ...    -0.028931 -0.013000 -0.060303   \n",
       "20    -0.281250 -0.059082  0.101074    ...     0.114258  0.191406 -0.207031   \n",
       "21    -0.136719  0.173828  0.182617    ...     0.015503  0.050049  0.069336   \n",
       "22    -0.198242  0.020752  0.118652    ...    -0.212891  0.043213 -0.139648   \n",
       "23     0.023071  0.080078  0.066406    ...     0.124512 -0.123535 -0.203125   \n",
       "24    -0.074707 -0.089355  0.218750    ...    -0.031250  0.044189 -0.098633   \n",
       "25    -0.079102 -0.084473  0.021973    ...    -0.120117  0.095215 -0.034912   \n",
       "26    -0.191406  0.162109  0.103027    ...     0.277344  0.087891 -0.059814   \n",
       "27    -0.113281  0.054443  0.057129    ...    -0.057129  0.152344 -0.031738   \n",
       "28    -0.061768  0.104004 -0.068848    ...     0.099121  0.029297 -0.095703   \n",
       "29     0.005188  0.131836  0.155273    ...     0.030151 -0.076172 -0.148438   \n",
       "...         ...       ...       ...    ...          ...       ...       ...   \n",
       "37755 -0.084900 -0.045939 -0.000023    ...    -0.069590 -0.067214 -0.088728   \n",
       "37756 -0.118652  0.242188  0.386719    ...    -0.339844 -0.076172  0.001915   \n",
       "37757 -0.029297  0.066895  0.241211    ...    -0.000152 -0.125977 -0.037109   \n",
       "37758 -0.084900 -0.045939 -0.000023    ...    -0.069590 -0.067214 -0.088728   \n",
       "37759 -0.084900 -0.045939 -0.000023    ...    -0.069590 -0.067214 -0.088728   \n",
       "37760 -0.140625  0.181641  0.343750    ...    -0.244141 -0.216797 -0.210938   \n",
       "37761 -0.134766 -0.423828  0.335938    ...    -0.004059  0.091797 -0.089844   \n",
       "37762  0.045898 -0.107422  0.105957    ...     0.084473  0.326172 -0.267578   \n",
       "37763  0.166016  0.235352  0.086426    ...     0.214844  0.016235 -0.117188   \n",
       "37764 -0.084900 -0.045939 -0.000023    ...    -0.069590 -0.067214 -0.088728   \n",
       "37765 -0.084900 -0.045939 -0.000023    ...    -0.069590 -0.067214 -0.088728   \n",
       "37766 -0.298828 -0.142578  0.012573    ...    -0.038330  0.201172 -0.671875   \n",
       "37767 -0.084900 -0.045939 -0.000023    ...    -0.069590 -0.067214 -0.088728   \n",
       "37768 -0.084900 -0.045939 -0.000023    ...    -0.069590 -0.067214 -0.088728   \n",
       "37769  0.084473 -0.107422  0.045898    ...    -0.031738  0.103027  0.017578   \n",
       "37770 -0.058838  0.011292 -0.002899    ...     0.027954 -0.023560 -0.073730   \n",
       "37771  0.103516  0.337891  0.118164    ...    -0.265625  0.175781  0.017456   \n",
       "37772 -0.084900 -0.045939 -0.000023    ...    -0.069590 -0.067214 -0.088728   \n",
       "37773  0.028198  0.023560  0.052002    ...     0.296875  0.125977 -0.051270   \n",
       "37774 -0.084900 -0.045939 -0.000023    ...    -0.069590 -0.067214 -0.088728   \n",
       "37775 -0.084900 -0.045939 -0.000023    ...    -0.069590 -0.067214 -0.088728   \n",
       "37776 -0.232422  0.343750 -0.490234    ...    -0.067383  0.294922  0.023560   \n",
       "37777 -0.084900 -0.045939 -0.000023    ...    -0.069590 -0.067214 -0.088728   \n",
       "37778  0.232422 -0.028198  0.241211    ...     0.046143  0.108887 -0.174805   \n",
       "37779 -0.084900 -0.045939 -0.000023    ...    -0.069590 -0.067214 -0.088728   \n",
       "37780 -0.208008 -0.107422  0.176758    ...    -0.111816 -0.028442 -0.062256   \n",
       "37781 -0.138672 -0.265625  0.205078    ...    -0.171875 -0.054688 -0.100586   \n",
       "37782 -0.084900 -0.045939 -0.000023    ...    -0.069590 -0.067214 -0.088728   \n",
       "37783 -0.084900 -0.045939 -0.000023    ...    -0.069590 -0.067214 -0.088728   \n",
       "37784  0.074219  0.215820  0.059814    ...    -0.010803  0.106934 -0.162109   \n",
       "\n",
       "            293       294       295       296       297       298       299  \n",
       "0      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1     -0.089867  0.088963 -0.081083  0.001657 -0.075659 -0.056583  0.052190  \n",
       "2      0.020630 -0.023315  0.000732 -0.092285 -0.021973  0.076660 -0.168945  \n",
       "3     -0.033203 -0.000938  0.031982  0.063477 -0.108887  0.048828 -0.130859  \n",
       "4      0.110840  0.208008  0.157227  0.061523 -0.109863 -0.008728  0.037109  \n",
       "5      0.165039 -0.022339 -0.064453  0.159180 -0.170898  0.196289 -0.099609  \n",
       "6     -0.074219 -0.042725 -0.057129 -0.207031  0.131836  0.084473  0.018677  \n",
       "7      0.096680 -0.012512  0.075684  0.004272 -0.126953  0.161133 -0.160156  \n",
       "8      0.035400 -0.086914 -0.081055  0.107910  0.003189  0.287109 -0.141602  \n",
       "9     -0.052979 -0.151367 -0.181641 -0.112305 -0.117188  0.168945 -0.051270  \n",
       "10     0.065430 -0.023193 -0.160156 -0.275391 -0.096191  0.080078 -0.075195  \n",
       "11    -0.162109 -0.038086 -0.063477 -0.103027 -0.003555 -0.062500 -0.055664  \n",
       "12     0.049561 -0.051758 -0.043457  0.119629  0.043945 -0.145508  0.071289  \n",
       "13     0.091309 -0.046631 -0.199219 -0.353516 -0.237305 -0.170898  0.023315  \n",
       "14     0.145508 -0.076172 -0.015625 -0.291016 -0.222656 -0.010376  0.071777  \n",
       "15     0.204102 -0.001801  0.017700  0.094727 -0.107910  0.166992 -0.169922  \n",
       "16    -0.008911 -0.129883 -0.034180  0.041260 -0.048584  0.294922 -0.202148  \n",
       "17     0.085449  0.103027 -0.318359 -0.079102  0.023804  0.060791 -0.121582  \n",
       "18     0.263672  0.003937 -0.205078 -0.392578 -0.070801  0.089844 -0.112305  \n",
       "19    -0.032715 -0.103516  0.044678 -0.095215 -0.015869  0.006714 -0.001884  \n",
       "20    -0.158203 -0.170898 -0.190430 -0.255859 -0.170898 -0.053467  0.188477  \n",
       "21    -0.015869 -0.046143 -0.265625 -0.011780 -0.086426  0.143555  0.027344  \n",
       "22    -0.011597  0.228516  0.170898 -0.010437 -0.094238 -0.172852 -0.052734  \n",
       "23     0.175781 -0.142578 -0.091797 -0.373047 -0.091309 -0.029175  0.037109  \n",
       "24     0.028564 -0.017090  0.108887  0.143555  0.081543 -0.045898 -0.046387  \n",
       "25    -0.038086 -0.077637 -0.055664  0.110352 -0.182617  0.044434 -0.041748  \n",
       "26     0.038086 -0.134766 -0.314453 -0.271484 -0.215820  0.061768 -0.184570  \n",
       "27     0.061035 -0.176758 -0.227539  0.113281 -0.011963  0.165039 -0.031738  \n",
       "28    -0.060059 -0.132812 -0.217773  0.016968 -0.186523  0.156250 -0.064941  \n",
       "29    -0.000231  0.118652 -0.135742 -0.034424  0.093750  0.027954 -0.115234  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "37755 -0.089867  0.088963 -0.081083  0.001657 -0.075659 -0.056583  0.052190  \n",
       "37756  0.040527 -0.094727 -0.008850 -0.081055 -0.162109  0.119141  0.226562  \n",
       "37757  0.123535  0.396484  0.175781  0.001007  0.169922  0.074707 -0.008484  \n",
       "37758 -0.089867  0.088963 -0.081083  0.001657 -0.075659 -0.056583  0.052190  \n",
       "37759 -0.089867  0.088963 -0.081083  0.001657 -0.075659 -0.056583  0.052190  \n",
       "37760 -0.181641  0.291016  0.255859  0.337891 -0.012268  0.141602  0.124512  \n",
       "37761  0.021973  0.332031  0.066895 -0.211914  0.040283 -0.267578 -0.063965  \n",
       "37762 -0.075195 -0.225586 -0.259766 -0.134766  0.083008  0.170898  0.232422  \n",
       "37763  0.067871  0.157227  0.162109 -0.052246  0.079590  0.120605 -0.009949  \n",
       "37764 -0.089867  0.088963 -0.081083  0.001657 -0.075659 -0.056583  0.052190  \n",
       "37765 -0.089867  0.088963 -0.081083  0.001657 -0.075659 -0.056583  0.052190  \n",
       "37766  0.320312  0.247070 -0.480469 -0.029785  0.064453  0.001274  0.029663  \n",
       "37767 -0.089867  0.088963 -0.081083  0.001657 -0.075659 -0.056583  0.052190  \n",
       "37768 -0.089867  0.088963 -0.081083  0.001657 -0.075659 -0.056583  0.052190  \n",
       "37769  0.043701 -0.367188  0.361328  0.003128 -0.028442 -0.022949  0.589844  \n",
       "37770  0.091797 -0.031494 -0.028687 -0.083496  0.053955  0.061523  0.009827  \n",
       "37771 -0.218750 -0.144531 -0.123535  0.110352 -0.140625 -0.294922  0.253906  \n",
       "37772 -0.089867  0.088963 -0.081083  0.001657 -0.075659 -0.056583  0.052190  \n",
       "37773 -0.130859 -0.108887 -0.106445  0.057373  0.082031 -0.111328  0.223633  \n",
       "37774 -0.089867  0.088963 -0.081083  0.001657 -0.075659 -0.056583  0.052190  \n",
       "37775 -0.089867  0.088963 -0.081083  0.001657 -0.075659 -0.056583  0.052190  \n",
       "37776  0.092285 -0.211914 -0.007629 -0.097168 -0.018555 -0.062012 -0.188477  \n",
       "37777 -0.089867  0.088963 -0.081083  0.001657 -0.075659 -0.056583  0.052190  \n",
       "37778  0.235352  0.355469  0.094727 -0.053467 -0.062012  0.082031 -0.050293  \n",
       "37779 -0.089867  0.088963 -0.081083  0.001657 -0.075659 -0.056583  0.052190  \n",
       "37780 -0.037842  0.011841  0.129883  0.210938 -0.212891 -0.197266  0.130859  \n",
       "37781 -0.088379  0.010925 -0.007446 -0.322266 -0.042236  0.081543  0.134766  \n",
       "37782 -0.089867  0.088963 -0.081083  0.001657 -0.075659 -0.056583  0.052190  \n",
       "37783 -0.089867  0.088963 -0.081083  0.001657 -0.075659 -0.056583  0.052190  \n",
       "37784 -0.166016  0.148438 -0.106934  0.209961  0.145508 -0.082520  0.151367  \n",
       "\n",
       "[37785 rows x 300 columns]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50333, 3)\n",
      "(1630, 7)\n"
     ]
    }
   ],
   "source": [
    "# One-hot vectors\n",
    "labels_train_3 = to_categorical(np.asarray(sentiments_train_3), 3) \n",
    "labels_train_7 = to_categorical(np.asarray(sentiments_train_7), 7)\n",
    "print(labels_train_3.shape)\n",
    "print(labels_train_7.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 35233 samples\n",
      "validation set: 15100 samples\n",
      "x_train_3 shape:  (35233, 32)\n",
      "y_train_3 shape:  (35233, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create and split dataset_3\n",
    "split_idx = int(len(data_train_3) * 0.70)\n",
    "\n",
    "x_train_3, x_val_3 = data_train_3[:split_idx], data_train_3[split_idx:]\n",
    "y_train_3, y_val_3 = labels_train_3[:split_idx], labels_train_3[split_idx:]\n",
    "\n",
    "print('training set: ' + str(len(x_train_3)) + ' samples')\n",
    "print('validation set: ' + str(len(x_val_3)) + ' samples')\n",
    "\n",
    "print('x_train_3 shape: ', x_train_3.shape)\n",
    "print('y_train_3 shape: ', y_train_3.shape)\n",
    "\n",
    "dataset_3 = x_train_3, y_train_3, x_val_3, y_val_3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 1385 samples\n",
      "validation set: 245 samples\n",
      "x_train_7 shape:  (1385, 32)\n",
      "y_train_7 shape:  (1385, 7)\n"
     ]
    }
   ],
   "source": [
    "# Create and split dataset_7\n",
    "split_idx = int(len(data_train_7) * 0.85)\n",
    "\n",
    "x_train_7, x_val_7 = data_train_7[:split_idx], data_train_7[split_idx:]\n",
    "y_train_7, y_val_7 = labels_train_7 [:split_idx], labels_train_7[split_idx:]\n",
    "\n",
    "print('training set: ' + str(len(x_train_7)) + ' samples')\n",
    "print('validation set: ' + str(len(x_val_7)) + ' samples')\n",
    "\n",
    "print('x_train_7 shape: ', x_train_7.shape)\n",
    "print('y_train_7 shape: ', y_train_7.shape)\n",
    "\n",
    "dataset_7 = x_train_7, y_train_7, x_val_7, y_val_7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A model factory class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "  \n",
    "  def build(self, layers, transferred_model):\n",
    "    # set a new model\n",
    "    if transferred_model is None: \n",
    "      self.model = Sequential()\n",
    "    else:\n",
    "      self.model = transferred_model\n",
    "      \n",
    "    # add/remove the layers\n",
    "    for layer in layers:\n",
    "      if layer[0] == 'Embedding':\n",
    "        self.model.add(layer[1])\n",
    "      if layer[0] == 'LSTM':\n",
    "        self.model.add(LSTM(layer[1]))\n",
    "      if layer[0] == 'GRU':\n",
    "        self.model.add(GRU(layer[1]))\n",
    "      if layer[0] == 'Dropout':\n",
    "        self.model.add(Dropout(layer[1]))\n",
    "      if layer[0] == 'Dense':\n",
    "        self.model.add(Dense(layer[1], activation=layer[2]))\n",
    "      if layer[0] == 'Cut':\n",
    "        for _ in range(layer[1]):\n",
    "          print(\"Cut\")\n",
    "          self.model.layers.pop()\n",
    "    \n",
    "    # compile the model, and show its summary\n",
    "    self.model.compile(loss=self.hp['loss'],\n",
    "                  optimizer=self.hp['optimizer'],\n",
    "                  metrics=self.hp['metrics'])\n",
    "    self.model.summary()\n",
    "  \n",
    "  def __init__(self, dataset, layers, hyperparameters, transferred_model=None):\n",
    "    self.x_train, self. y_train, self.x_val, self.y_val = dataset\n",
    "    self.hp = hyperparameters\n",
    "    self.build(layers, transferred_model) # build the model (set self.model accordingly)\n",
    "    \n",
    "  def fit(self):\n",
    "    self.model.fit(self.x_train, self.y_train, validation_data=(self.x_val, self.y_val),\n",
    "                        epochs=self.hp['epochs'], batch_size=self.hp['batch_size'])\n",
    "\n",
    "  def predict(self, data_test):\n",
    "     self.model.predict(data_test)\n",
    "    \n",
    "  def save(self, path):\n",
    "    self.model.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False,\n",
    "                            name='embedding_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training on dataset_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 32, 300)           11335500  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 11,379,279\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 11,335,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model 1\n",
    "layers_1 = [\n",
    "  ('Embedding', embedding_layer),\n",
    "  ('LSTM', 32),\n",
    "  ('Dropout', 0.2),\n",
    "  ('Dense', 32, 'relu'),\n",
    "  ('Dropout', 0.2),\n",
    "  ('Dense', 3, 'softmax')\n",
    "]\n",
    "\n",
    "hyperparameters_1 = {\n",
    "  'loss': 'categorical_crossentropy',\n",
    "  'optimizer': 'Adam', \n",
    "  'metrics': ['acc'],\n",
    "  'epochs': 1, # FIXME\n",
    "  'batch_size': 50 \n",
    "}\n",
    "\n",
    "model1 = Model(dataset_3, layers_1, hyperparameters_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 32, 300)           11335500  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                31968     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 11,367,567\n",
      "Trainable params: 32,067\n",
      "Non-trainable params: 11,335,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model 2\n",
    "layers_2 = [\n",
    "  ('Embedding', embedding_layer),\n",
    "  ('GRU', 32),\n",
    "  ('Dropout', 0.2),\n",
    "  ('Dense', 3, 'softmax')\n",
    "]\n",
    "\n",
    "hyperparameters_2 = {\n",
    "  'loss': 'categorical_crossentropy',\n",
    "  'optimizer': 'rmsprop', \n",
    "  'metrics': ['acc'],\n",
    "  'epochs': 6,\n",
    "  'batch_size': 50 \n",
    "}\n",
    "\n",
    "model2 = Model(dataset_3, layers_2, hyperparameters_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35233 samples, validate on 15100 samples\n",
      "Epoch 1/1\n",
      "35233/35233 [==============================] - 20s 569us/step - loss: 0.8419 - acc: 0.6090 - val_loss: 0.8201 - val_acc: 0.6017\n"
     ]
    }
   ],
   "source": [
    "# Model call (training + save)\n",
    "model1.fit()\n",
    "model1.save(\"./model1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training on dataset_7 by transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 32, 300)           11335500  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 11,379,279\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 11,335,500\n",
      "_________________________________________________________________\n",
      "Cut\n",
      "Cut\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 32, 300)           11335500  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 150)               600       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                9664      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 11,389,998\n",
      "Trainable params: 54,498\n",
      "Non-trainable params: 11,335,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model built from transfer learning\n",
    "\n",
    "transferred_model = load_model(\"./model1.h5\") #or: model1.model\n",
    "transferred_model.summary()\n",
    "\n",
    "layers = [\n",
    "  ('Cut', 2),\n",
    "  ('Dense', 150, 'relu'),\n",
    "  ('Dense', 64, 'relu'),\n",
    "  ('Dense', 7, 'softmax'),\n",
    "]\n",
    "\n",
    "hyperparameters = {\n",
    "  'loss': 'categorical_crossentropy',\n",
    "  'optimizer': 'rmsprop', \n",
    "  'metrics': ['acc'],\n",
    "  'epochs': 11,\n",
    "  'batch_size': 50\n",
    "}\n",
    "\n",
    "model = Model(dataset_7, layers, hyperparameters, transferred_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1385 samples, validate on 245 samples\n",
      "Epoch 1/11\n",
      "1385/1385 [==============================] - 3s 2ms/step - loss: 1.8253 - acc: 0.2780 - val_loss: 1.7866 - val_acc: 0.2571\n",
      "Epoch 2/11\n",
      "1385/1385 [==============================] - 1s 849us/step - loss: 1.7004 - acc: 0.3329 - val_loss: 1.7166 - val_acc: 0.2694\n",
      "Epoch 3/11\n",
      "1385/1385 [==============================] - 1s 869us/step - loss: 1.6408 - acc: 0.3466 - val_loss: 1.7067 - val_acc: 0.2776\n",
      "Epoch 4/11\n",
      "1385/1385 [==============================] - 1s 859us/step - loss: 1.6067 - acc: 0.3466 - val_loss: 1.7022 - val_acc: 0.2735\n",
      "Epoch 5/11\n",
      "1385/1385 [==============================] - 1s 866us/step - loss: 1.5878 - acc: 0.3523 - val_loss: 1.6929 - val_acc: 0.2776\n",
      "Epoch 6/11\n",
      "1385/1385 [==============================] - 1s 882us/step - loss: 1.5578 - acc: 0.3639 - val_loss: 1.6797 - val_acc: 0.2735\n",
      "Epoch 7/11\n",
      "1385/1385 [==============================] - 1s 874us/step - loss: 1.5459 - acc: 0.3755 - val_loss: 1.6768 - val_acc: 0.3429\n",
      "Epoch 8/11\n",
      "1385/1385 [==============================] - 1s 862us/step - loss: 1.5253 - acc: 0.3783 - val_loss: 1.7033 - val_acc: 0.3224\n",
      "Epoch 9/11\n",
      "1385/1385 [==============================] - 1s 872us/step - loss: 1.5024 - acc: 0.4043 - val_loss: 1.6899 - val_acc: 0.3347\n",
      "Epoch 10/11\n",
      "1385/1385 [==============================] - 1s 861us/step - loss: 1.4832 - acc: 0.4014 - val_loss: 1.7214 - val_acc: 0.3184\n",
      "Epoch 11/11\n",
      "1385/1385 [==============================] - 1s 870us/step - loss: 1.4534 - acc: 0.4245 - val_loss: 1.7161 - val_acc: 0.3388\n"
     ]
    }
   ],
   "source": [
    "model.fit()\n",
    "model.save(\"./model3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a59ff2f147db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                \"3: very positive emotional state can be inferred\"]\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Intensity Class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconclusions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "r = model.predict(data_test_7)\n",
    "\n",
    "conclusions = [\"-1: slightly negative emotional state can be inferred\",\n",
    "               \"-2: moderately negative emotional state can be inferred\",\n",
    "               \"-3: very negative emotional state can be inferred\",\n",
    "               \"0: neutral or mixed emotional state can be inferred\",\n",
    "               \"1: slightly positive emotional state can be inferred\",\n",
    "               \"2: moderately positive emotional state can be inferred\",\n",
    "               \"3: very positive emotional state can be inferred\"]\n",
    "\n",
    "for i in range(len(r)):\n",
    "    data['Intensity Class'][i] = conclusions[numpy.argmax(r[i])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: write predictions with Pandas instead\n",
    "\n",
    "with open(\"./submission_after.csv\", \"w\") as f:\n",
    "  f.write(\"ID\\tTweet\\tAffect Dimension\\tIntensity Class\\n\")\n",
    "  for d in range(len(data)):\n",
    "    f.write(data['ID'][d] + \"\\t\" + data['Tweet'][d] + \"\\tvalence\\t\" + data['Intensity Class'][d] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 32, 300)           11335500  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 11,379,279\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 11,335,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"./model1.h5\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x7f36f4a46160>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers.pop()\n",
    "model.layers.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 32, 300)           11335500  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 11,379,279\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 11,335,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.outputs = [model.layers[-1].output]\n",
    "model.layers[-1].outbound_nodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 32, 300)           11335500  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 11,379,279\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 11,335,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 32, 300)           11335500  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 11,379,279\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 11,335,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
